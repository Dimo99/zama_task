For coding the whole task I was heavily using claude code. I started with basic structure just defining SQLite tables tokens and transfers an RPC to fetch the current_block, the block a contract was deployed at and a loop that fetches logs from the block the contract was deployed to the current block and once that is reached we are just going to fetch logs again on every 12s (ETH Mainnet block time). I started with very ambitious token USDC so I saw after few thousands maybe million events got processed that this is not going to work the first thing that I improved is I made the insertion work in parallel to the fetching so I created an insertion worker task that received the batches it needs to insert via a channel this way I could continue to fetch logs when already received logs were getting inserted and also I had the guarantee that the insertion will happen in the same order I sent the tasks for insertion. Ofcourse only that was not enough so the next thing was to enable concurent fetching the way this was done was that on some rate limiting interval I was firing a new request and adding it in futures ordered (up to MAX_PENDING_REQUESTS). This way I could fire many requests in paralel and process them in the correct order just don't wait for the first request to finish to fire the second one. But just that was also not enough I decided to implement the functionality of having multiple RPC clients so I can fire requests to multiple RPCs at the same time those bybising some rate limiting and also perfomence issues when you overload a single RPC with requests (I found out you can make some of the public RPCs hang and timeout on requests when you overload it) that is when I also introduced a timeout feature so requests will try again after a configurable timeout. Also another issue was that ofcourse usdc could have many thousands of events in a 1000 blocks which was the interval I was processing so I saw that RPCs fail with query exceeds max results {}, retry with the range {}-{} so I implemented a processing logic for this case. Then I continued with the querry tool where I realized that things like calculating the balance for certain addresses that have made thousands of transactions is again pretty slow so the idea of denomarmalization came to my mind. But before that I needed to implement finality so I implemented the finality in a way that on every epoch which is 32 blocks in ethereum I will fetch the newly finalized block update the last_processed_finalized_block in the db and refetch all events from last_processed_finalized_block to current_finalized_block check for changes by just checking if there is a difference in the blockhashes or if there are events on chain that are not existing in the db or vice versa. Correct those changes and update is_finalized flag in the db. Then this needed to be added to the querry tool as an optional parameter to only querry finalized transfers. And finaly the denormalization was done with a balance table with address, balance which gets updated whenever we add finalized transfer for efficient top-holders and balance reports. There was interesting bug there as I first implemented the addition with wrapping_add and wrapping_sub thinking that everything will be correct but for USDC for some reason I haven't investigated I've got a lot of addresses that have wrapped arround so I changed the code to use `saturating_add` and `saturating_sub` so this wrapping will be an interesting thing to investigate and also the whole project doesn't have a single test. It will be very usefull to add tests as next steps